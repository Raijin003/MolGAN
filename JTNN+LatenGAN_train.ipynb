{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JTNN+LatenGAN_train",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k85YYlp3wi9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "9f2f79b7-4d98-4008-cdbe-11479e946cbf"
      },
      "source": [
        "!wget http://molcyclegan.ardigen.com/250k_rndm_zinc_drugs_clean_3_canonized.csv\n",
        "!wget http://molcyclegan.ardigen.com/X_JTVAE_250k_rndm_zinc.csv"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-15 18:16:16--  http://molcyclegan.ardigen.com/250k_rndm_zinc_drugs_clean_3_canonized.csv\n",
            "Resolving molcyclegan.ardigen.com (molcyclegan.ardigen.com)... 188.128.194.238\n",
            "Connecting to molcyclegan.ardigen.com (molcyclegan.ardigen.com)|188.128.194.238|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22101155 (21M) [text/csv]\n",
            "Saving to: ‘250k_rndm_zinc_drugs_clean_3_canonized.csv.1’\n",
            "\n",
            "250k_rndm_zinc_drug 100%[===================>]  21.08M  8.29MB/s    in 2.5s    \n",
            "\n",
            "2020-03-15 18:16:19 (8.29 MB/s) - ‘250k_rndm_zinc_drugs_clean_3_canonized.csv.1’ saved [22101155/22101155]\n",
            "\n",
            "--2020-03-15 18:16:19--  http://molcyclegan.ardigen.com/X_JTVAE_250k_rndm_zinc.csv\n",
            "Resolving molcyclegan.ardigen.com (molcyclegan.ardigen.com)... 188.128.194.238\n",
            "Connecting to molcyclegan.ardigen.com (molcyclegan.ardigen.com)|188.128.194.238|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 164441376 (157M) [text/csv]\n",
            "Saving to: ‘X_JTVAE_250k_rndm_zinc.csv.1’\n",
            "\n",
            "X_JTVAE_250k_rndm_z 100%[===================>] 156.82M  11.4MB/s    in 16s     \n",
            "\n",
            "2020-03-15 18:16:35 (9.95 MB/s) - ‘X_JTVAE_250k_rndm_zinc.csv.1’ saved [164441376/164441376]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjywBaDEVFCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cTrQarwwOk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('X_JTVAE_250k_rndm_zinc.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxlFYRMHTPf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smiles = data['SMILES'].values\n",
        "np.savetxt(r'smiles.txt', smiles, fmt='%s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh-408HFrktC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, data_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.data_shape = data_shape\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(self.data_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, mol):\n",
        "        validity = self.model(mol)\n",
        "        return validity\n",
        "\n",
        "    def save(self, path):\n",
        "        save_dict = {\n",
        "            'model': self.model.state_dict(),\n",
        "            'data_shape': self.data_shape,\n",
        "        }\n",
        "        torch.save(save_dict, path)\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        save_dict = torch.load(path)\n",
        "        D = Discriminator(save_dict['data_shape'])\n",
        "        D.model.load_state_dict(save_dict[\"model\"])\n",
        "\n",
        "        return D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0ZmwIN5VPeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, data_shape, latent_dim=None):\n",
        "        super(Generator, self).__init__()\n",
        "        self.data_shape = data_shape\n",
        "\n",
        "        # latent dim of the generator is one of the hyperparams.\n",
        "        # by default it is set to the prod of data_shapes\n",
        "        self.latent_dim = int(np.prod(self.data_shape)) if latent_dim is None else latent_dim\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(self.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.data_shape))),\n",
        "            # nn.Tanh() # expecting latent vectors to be not normalized\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.model(z)\n",
        "        return out\n",
        "\n",
        "    def save(self, path):\n",
        "        save_dict = {\n",
        "            'latent_dim': self.latent_dim,\n",
        "            'model': self.model.state_dict(),\n",
        "            'data_shape': self.data_shape,\n",
        "        }\n",
        "        torch.save(save_dict, path)\n",
        "\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        save_dict = torch.load(path)\n",
        "        G = Generator(save_dict['data_shape'], latent_dim=save_dict['latent_dim'])\n",
        "        G.model.load_state_dict(save_dict[\"model\"])\n",
        "\n",
        "        return G"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_NWvRUvVS5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampler(object):\n",
        "    \"\"\"\n",
        "    Sampling the mols the generator.\n",
        "    All scripts should use this class for sampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, generator: Generator):\n",
        "        self.set_generator(generator)\n",
        "\n",
        "    def set_generator(self, generator):\n",
        "        self.G = generator\n",
        "\n",
        "    def sample(self, n):\n",
        "        # Sample noise as generator input\n",
        "        z = torch.cuda.FloatTensor(np.random.uniform(-1, 1, (n, self.G.latent_dim)))\n",
        "        # Generate a batch of mols\n",
        "        return self.G(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlo1tKSDAuHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LatentMolsDataset(data.Dataset):\n",
        "    def __init__(self, latent_space_mols):\n",
        "        self.data = latent_space_mols\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xjPf5FRV4xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "class TrainModelRunner:\n",
        "    # Loss weight for gradient penalty\n",
        "    lambda_gp = 10\n",
        "\n",
        "    def __init__(self, input_data_path, output_model_folder, decode_mols_save_path='', n_epochs=2000, starting_epoch=1,\n",
        "                 batch_size=2500, lr=0.0002, b1=0.5, b2=0.999,  n_critic=5,\n",
        "                 save_interval=1000, sample_after_training=30000, message=\"\"):\n",
        "        self.message = message\n",
        "\n",
        "        # init params\n",
        "        self.input_data_path = input_data_path\n",
        "        self.output_model_folder = output_model_folder\n",
        "        self.n_epochs = n_epochs\n",
        "        self.starting_epoch = starting_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.b1 = b1\n",
        "        self.b2 = b2\n",
        "        self.n_critic = n_critic\n",
        "        self.save_interval = save_interval\n",
        "        self.sample_after_training = sample_after_training\n",
        "        self.decode_mols_save_path = decode_mols_save_path\n",
        "\n",
        "        # initialize dataloader\n",
        "        smiles_lat = pd.read_csv(input_data_path)\n",
        "        latent_space_mols = smiles_lat.drop('SMILES', axis=1).values\n",
        "        latent_space_mols = latent_space_mols.reshape(latent_space_mols.shape[0], 56)\n",
        "\n",
        "        self.dataloader = torch.utils.data.DataLoader(LatentMolsDataset(latent_space_mols), shuffle=True,\n",
        "                                                      batch_size=self.batch_size, drop_last=True)\n",
        "\n",
        "        # load discriminator\n",
        "        discriminator_name = 'discriminator.txt' if self.starting_epoch == 1 else str(\n",
        "            self.starting_epoch) + '_discriminator.txt'\n",
        "        discriminator_path = os.path.join(output_model_folder, discriminator_name)\n",
        "        self.D = Discriminator.load(discriminator_path)\n",
        "        # self.D = Discriminator(latent_space_mols[0].shape)\n",
        "        # load generator\n",
        "        generator_name = 'generator.txt' if self.starting_epoch == 1 else str(\n",
        "            self.starting_epoch) + '_generator.txt'\n",
        "        generator_path = os.path.join(output_model_folder, generator_name)\n",
        "        self.G = Generator(.load(generator_path)\n",
        "        # self.G = Generator(latent_space_mols[0].shape)\n",
        "        # initialize sampler\n",
        "        self.Sampler = Sampler(self.G)\n",
        "\n",
        "        # initialize optimizer\n",
        "        self.optimizer_G = torch.optim.Adam(self.G.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
        "        self.optimizer_D = torch.optim.Adam(self.D.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
        "\n",
        "        # Tensor\n",
        "        cuda = True if torch.cuda.is_available() else False\n",
        "        if cuda:\n",
        "            self.G.cuda()\n",
        "            self.D.cuda()\n",
        "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        print(\"Run began.\")\n",
        "        print(\"Message: %s\" % self.message)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        batches_done = 0\n",
        "        disc_loss_log = []\n",
        "        g_loss_log = []\n",
        "\n",
        "        for epoch in range(self.starting_epoch, self.n_epochs + self.starting_epoch):\n",
        "            disc_loss_per_batch = []\n",
        "            g_loss_log_per_batch = []\n",
        "            for i, real_mols in enumerate(self.dataloader):\n",
        "\n",
        "                # Configure input\n",
        "                real_mols = real_mols.type(self.Tensor)\n",
        "                # real_mols = np.squeeze(real_mols, axis=1)\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                self.optimizer_D.zero_grad()\n",
        "\n",
        "                # Generate a batch of mols from noise\n",
        "                fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
        "\n",
        "                # Real mols\n",
        "                real_validity = self.D(real_mols)\n",
        "                # Fake mols\n",
        "                fake_validity = self.D(fake_mols)\n",
        "                # Gradient penalty\n",
        "                gradient_penalty = self.compute_gradient_penalty(real_mols.data, fake_mols.data)\n",
        "                # Adversarial loss\n",
        "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + self.lambda_gp * gradient_penalty\n",
        "                disc_loss_per_batch.append(d_loss.item())\n",
        "\n",
        "                d_loss.backward()\n",
        "                self.optimizer_D.step()\n",
        "                self.optimizer_G.zero_grad()\n",
        "\n",
        "                # Train the generator every n_critic steps\n",
        "                if i % self.n_critic == 0:\n",
        "                    # -----------------\n",
        "                    #  Train Generator\n",
        "                    # -----------------\n",
        "\n",
        "                    # Generate a batch of mols\n",
        "                    fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
        "                    # Loss measures generator's ability to fool the discriminator\n",
        "                    # Train on fake images\n",
        "                    fake_validity = self.D(fake_mols)\n",
        "                    g_loss = -torch.mean(fake_validity)\n",
        "                    g_loss_log_per_batch.append(g_loss.item())\n",
        "\n",
        "                    g_loss.backward()\n",
        "                    self.optimizer_G.step()\n",
        "\n",
        "                    batches_done += self.n_critic\n",
        "\n",
        "                # If last batch in the set\n",
        "                if i == len(self.dataloader) - 1:\n",
        "                    if epoch % self.save_interval == 0:\n",
        "                        generator_save_path = os.path.join(self.output_model_folder,\n",
        "                                                           str(epoch) + '_generator.txt')\n",
        "                        discriminator_save_path = os.path.join(self.output_model_folder,\n",
        "                                                               str(epoch) + '_discriminator.txt')\n",
        "                        self.G.save(generator_save_path)\n",
        "                        self.D.save(discriminator_save_path)\n",
        "\n",
        "                    disc_loss_log.append([time.time(), epoch, np.mean(disc_loss_per_batch)])\n",
        "                    g_loss_log.append([time.time(), epoch, np.mean(g_loss_log_per_batch)])\n",
        "\n",
        "                    # Print and log\n",
        "                    print(\n",
        "                        \"[Epoch %d/%d]  [Disc loss: %f] [Gen loss: %f] \"\n",
        "                        % (epoch, self.n_epochs + self.starting_epoch, disc_loss_log[-1][2], g_loss_log[-1][2])\n",
        "                    )\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "        # log the losses\n",
        "        with open(os.path.join(self.output_model_folder, 'disc_loss.json'), 'w') as json_file:\n",
        "            json.dump(disc_loss_log, json_file)\n",
        "        with open(os.path.join(self.output_model_folder, 'gen_loss.json'), 'w') as json_file:\n",
        "            json.dump(g_loss_log, json_file)\n",
        "\n",
        "        # Sampling after training\n",
        "        if self.sample_after_training > 0:\n",
        "            print(\"Training finished. Generating sample of latent vectors\")\n",
        "            # sampling mode\n",
        "            torch.no_grad()\n",
        "            self.G.eval()\n",
        "\n",
        "            S = Sampler(generator=self.G)\n",
        "            latent = S.sample(self.sample_after_training)\n",
        "            latent = latent.detach().cpu().numpy().tolist()\n",
        "\n",
        "            sampled_mols_save_path = os.path.join(self.output_model_folder, 'sampled')\n",
        "            np.save(sampled_mols_save_path+f'_epoch{epoch}', latent)\n",
        "\n",
        "            # decoding sampled mols\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
        "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "        # Random weight term for interpolation between real and fake samples\n",
        "        alpha = self.Tensor(np.random.random((real_samples.size(0), 1)))\n",
        "\n",
        "        # Get random interpolation between real and fake samples\n",
        "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "        d_interpolates = self.D(interpolates)\n",
        "        fake = self.Tensor(real_samples.shape[0], 1).fill_(1.0)\n",
        "\n",
        "        # Get gradient w.r.t. interpolates\n",
        "        gradients = autograd.grad(\n",
        "            outputs=d_interpolates,\n",
        "            inputs=interpolates,\n",
        "            grad_outputs=fake,\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "            only_inputs=True,\n",
        "        )[0]\n",
        "        gradients = gradients.view(gradients.size(0), -1)\n",
        "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "        return gradient_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGNNUWy1Az_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = TrainModelRunner('/content/X_JTVAE_250k_rndm_zinc.csv', output_model_folder='/content/model', starting_epoch=0,\n",
        "                           save_interval=100, message='Starting training', batch_size=2500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAV2y8c_kHWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTARoSEVlICZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.no_grad()\n",
        "trainer.G.eval()\n",
        "\n",
        "S = Sampler(generator=trainer.G)\n",
        "latent = S.sample(10) #10 samples\n",
        "latent = latent.detach().cpu().numpy().tolist()\n",
        "\n",
        "sampled_mols_save_path = os.path.join(trainer.output_model_folder, 'sampled')\n",
        "np.save(sampled_mols_save_path+f'_epoch{200}', latent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1VyOt8pH0Dn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "bdb22d28-0e7c-45a2-ecc4-cc2d6a73b7ff"
      },
      "source": [
        "x = np.load('/content/model/sampled_epoch200.npy')\n",
        "pd.DataFrame(x).head(10)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.206029</td>\n",
              "      <td>-0.808622</td>\n",
              "      <td>0.227027</td>\n",
              "      <td>-1.398846</td>\n",
              "      <td>-0.100072</td>\n",
              "      <td>2.809301</td>\n",
              "      <td>0.433412</td>\n",
              "      <td>0.617868</td>\n",
              "      <td>2.921274</td>\n",
              "      <td>-0.630039</td>\n",
              "      <td>-0.015929</td>\n",
              "      <td>-1.214874</td>\n",
              "      <td>1.238661</td>\n",
              "      <td>1.919663</td>\n",
              "      <td>1.712633</td>\n",
              "      <td>-2.118173</td>\n",
              "      <td>2.056915</td>\n",
              "      <td>-1.282244</td>\n",
              "      <td>-0.323812</td>\n",
              "      <td>-1.917734</td>\n",
              "      <td>-1.834210</td>\n",
              "      <td>1.901669</td>\n",
              "      <td>1.027108</td>\n",
              "      <td>-1.074955</td>\n",
              "      <td>0.722044</td>\n",
              "      <td>-0.772840</td>\n",
              "      <td>-1.178263</td>\n",
              "      <td>1.204141</td>\n",
              "      <td>0.021915</td>\n",
              "      <td>-0.317627</td>\n",
              "      <td>-0.220762</td>\n",
              "      <td>0.052131</td>\n",
              "      <td>0.456755</td>\n",
              "      <td>0.070525</td>\n",
              "      <td>0.558844</td>\n",
              "      <td>0.530643</td>\n",
              "      <td>0.298532</td>\n",
              "      <td>-0.741765</td>\n",
              "      <td>-0.120004</td>\n",
              "      <td>-0.887192</td>\n",
              "      <td>0.504271</td>\n",
              "      <td>1.033725</td>\n",
              "      <td>0.036540</td>\n",
              "      <td>0.565099</td>\n",
              "      <td>-0.040974</td>\n",
              "      <td>-0.371490</td>\n",
              "      <td>0.317552</td>\n",
              "      <td>1.124570</td>\n",
              "      <td>-1.216062</td>\n",
              "      <td>-0.349802</td>\n",
              "      <td>0.016857</td>\n",
              "      <td>-0.316001</td>\n",
              "      <td>0.645152</td>\n",
              "      <td>-1.070026</td>\n",
              "      <td>-0.285900</td>\n",
              "      <td>0.443744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.682056</td>\n",
              "      <td>-0.609567</td>\n",
              "      <td>-2.252846</td>\n",
              "      <td>1.211958</td>\n",
              "      <td>-1.366811</td>\n",
              "      <td>2.314502</td>\n",
              "      <td>-1.487820</td>\n",
              "      <td>1.330656</td>\n",
              "      <td>0.508410</td>\n",
              "      <td>2.560220</td>\n",
              "      <td>2.649158</td>\n",
              "      <td>3.739150</td>\n",
              "      <td>-0.219646</td>\n",
              "      <td>0.185257</td>\n",
              "      <td>2.063104</td>\n",
              "      <td>-0.191439</td>\n",
              "      <td>1.270549</td>\n",
              "      <td>0.951931</td>\n",
              "      <td>-0.371398</td>\n",
              "      <td>4.485538</td>\n",
              "      <td>-1.377823</td>\n",
              "      <td>-0.388842</td>\n",
              "      <td>-1.598731</td>\n",
              "      <td>-0.444010</td>\n",
              "      <td>-2.914255</td>\n",
              "      <td>1.085253</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>-0.626002</td>\n",
              "      <td>0.120057</td>\n",
              "      <td>-0.377855</td>\n",
              "      <td>0.867117</td>\n",
              "      <td>0.566207</td>\n",
              "      <td>0.422510</td>\n",
              "      <td>-0.635424</td>\n",
              "      <td>0.363804</td>\n",
              "      <td>0.223369</td>\n",
              "      <td>-0.074575</td>\n",
              "      <td>0.139140</td>\n",
              "      <td>0.039169</td>\n",
              "      <td>-0.178168</td>\n",
              "      <td>-0.175273</td>\n",
              "      <td>0.185514</td>\n",
              "      <td>-0.198111</td>\n",
              "      <td>0.602899</td>\n",
              "      <td>0.084276</td>\n",
              "      <td>-1.319473</td>\n",
              "      <td>0.293325</td>\n",
              "      <td>0.157579</td>\n",
              "      <td>-0.099214</td>\n",
              "      <td>0.164480</td>\n",
              "      <td>-0.283875</td>\n",
              "      <td>0.160912</td>\n",
              "      <td>0.265407</td>\n",
              "      <td>0.191268</td>\n",
              "      <td>-0.078609</td>\n",
              "      <td>0.022873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.058921</td>\n",
              "      <td>-1.012089</td>\n",
              "      <td>-4.200319</td>\n",
              "      <td>0.859445</td>\n",
              "      <td>1.797249</td>\n",
              "      <td>1.589210</td>\n",
              "      <td>-3.770724</td>\n",
              "      <td>0.441195</td>\n",
              "      <td>1.834026</td>\n",
              "      <td>-1.979702</td>\n",
              "      <td>-0.520992</td>\n",
              "      <td>2.860998</td>\n",
              "      <td>-2.348060</td>\n",
              "      <td>-0.369602</td>\n",
              "      <td>-1.072005</td>\n",
              "      <td>-4.138757</td>\n",
              "      <td>0.461854</td>\n",
              "      <td>-0.450401</td>\n",
              "      <td>-0.299388</td>\n",
              "      <td>-0.568555</td>\n",
              "      <td>-1.126057</td>\n",
              "      <td>-0.018332</td>\n",
              "      <td>3.364588</td>\n",
              "      <td>-2.255006</td>\n",
              "      <td>-2.616036</td>\n",
              "      <td>-0.795993</td>\n",
              "      <td>1.292032</td>\n",
              "      <td>-1.284919</td>\n",
              "      <td>-0.466509</td>\n",
              "      <td>-0.498870</td>\n",
              "      <td>0.477180</td>\n",
              "      <td>-0.266820</td>\n",
              "      <td>1.612430</td>\n",
              "      <td>0.253728</td>\n",
              "      <td>0.177948</td>\n",
              "      <td>-0.054492</td>\n",
              "      <td>0.499810</td>\n",
              "      <td>0.103870</td>\n",
              "      <td>-0.225704</td>\n",
              "      <td>-0.456521</td>\n",
              "      <td>0.024753</td>\n",
              "      <td>0.131566</td>\n",
              "      <td>-0.312732</td>\n",
              "      <td>0.850896</td>\n",
              "      <td>0.593622</td>\n",
              "      <td>-0.478496</td>\n",
              "      <td>0.323845</td>\n",
              "      <td>-0.067242</td>\n",
              "      <td>-0.157220</td>\n",
              "      <td>0.246735</td>\n",
              "      <td>-0.273384</td>\n",
              "      <td>0.359672</td>\n",
              "      <td>0.109671</td>\n",
              "      <td>0.271430</td>\n",
              "      <td>0.086816</td>\n",
              "      <td>0.233399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.496926</td>\n",
              "      <td>3.138901</td>\n",
              "      <td>-2.547379</td>\n",
              "      <td>-0.869359</td>\n",
              "      <td>-0.259328</td>\n",
              "      <td>1.416225</td>\n",
              "      <td>-0.994070</td>\n",
              "      <td>-0.486854</td>\n",
              "      <td>0.142025</td>\n",
              "      <td>0.237491</td>\n",
              "      <td>-1.986327</td>\n",
              "      <td>0.143168</td>\n",
              "      <td>0.062135</td>\n",
              "      <td>-0.776502</td>\n",
              "      <td>1.883481</td>\n",
              "      <td>-0.397742</td>\n",
              "      <td>-1.397495</td>\n",
              "      <td>-0.964522</td>\n",
              "      <td>-1.808160</td>\n",
              "      <td>1.430580</td>\n",
              "      <td>2.632986</td>\n",
              "      <td>-0.582195</td>\n",
              "      <td>0.175714</td>\n",
              "      <td>-1.508701</td>\n",
              "      <td>-1.337555</td>\n",
              "      <td>2.593554</td>\n",
              "      <td>-0.872461</td>\n",
              "      <td>3.568985</td>\n",
              "      <td>0.134516</td>\n",
              "      <td>-0.299169</td>\n",
              "      <td>0.990468</td>\n",
              "      <td>-0.287957</td>\n",
              "      <td>0.345039</td>\n",
              "      <td>-0.259158</td>\n",
              "      <td>0.078833</td>\n",
              "      <td>0.049462</td>\n",
              "      <td>0.319563</td>\n",
              "      <td>-0.355350</td>\n",
              "      <td>0.452621</td>\n",
              "      <td>-0.530031</td>\n",
              "      <td>-0.214212</td>\n",
              "      <td>0.212138</td>\n",
              "      <td>-0.075205</td>\n",
              "      <td>0.615783</td>\n",
              "      <td>-0.042233</td>\n",
              "      <td>-0.567815</td>\n",
              "      <td>0.207993</td>\n",
              "      <td>0.016102</td>\n",
              "      <td>-0.346160</td>\n",
              "      <td>0.084241</td>\n",
              "      <td>-0.218341</td>\n",
              "      <td>0.211247</td>\n",
              "      <td>0.210123</td>\n",
              "      <td>-0.039403</td>\n",
              "      <td>-0.195077</td>\n",
              "      <td>-0.499018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.791522</td>\n",
              "      <td>-4.199571</td>\n",
              "      <td>-4.615912</td>\n",
              "      <td>-0.837690</td>\n",
              "      <td>1.186579</td>\n",
              "      <td>-0.089598</td>\n",
              "      <td>-1.146151</td>\n",
              "      <td>4.647312</td>\n",
              "      <td>1.291135</td>\n",
              "      <td>-0.028676</td>\n",
              "      <td>-1.718559</td>\n",
              "      <td>-1.847364</td>\n",
              "      <td>-1.370815</td>\n",
              "      <td>-1.421265</td>\n",
              "      <td>2.017358</td>\n",
              "      <td>-0.440322</td>\n",
              "      <td>-1.473093</td>\n",
              "      <td>-2.497284</td>\n",
              "      <td>-2.246482</td>\n",
              "      <td>0.538316</td>\n",
              "      <td>-1.227640</td>\n",
              "      <td>-0.100853</td>\n",
              "      <td>-1.246739</td>\n",
              "      <td>-2.504039</td>\n",
              "      <td>-0.146660</td>\n",
              "      <td>2.601627</td>\n",
              "      <td>1.545972</td>\n",
              "      <td>2.326550</td>\n",
              "      <td>-0.976732</td>\n",
              "      <td>-0.191328</td>\n",
              "      <td>0.557248</td>\n",
              "      <td>0.556218</td>\n",
              "      <td>5.861549</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.195171</td>\n",
              "      <td>0.479189</td>\n",
              "      <td>0.460419</td>\n",
              "      <td>-0.287761</td>\n",
              "      <td>-0.376164</td>\n",
              "      <td>-0.515892</td>\n",
              "      <td>0.411546</td>\n",
              "      <td>0.644988</td>\n",
              "      <td>-0.570256</td>\n",
              "      <td>0.536020</td>\n",
              "      <td>0.870963</td>\n",
              "      <td>-0.239492</td>\n",
              "      <td>0.083884</td>\n",
              "      <td>0.099846</td>\n",
              "      <td>0.012018</td>\n",
              "      <td>0.201893</td>\n",
              "      <td>0.340828</td>\n",
              "      <td>1.037974</td>\n",
              "      <td>-0.435755</td>\n",
              "      <td>-0.107969</td>\n",
              "      <td>0.086815</td>\n",
              "      <td>0.554617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.041741</td>\n",
              "      <td>0.082437</td>\n",
              "      <td>0.549398</td>\n",
              "      <td>-2.424614</td>\n",
              "      <td>0.661732</td>\n",
              "      <td>3.692330</td>\n",
              "      <td>0.298974</td>\n",
              "      <td>1.701455</td>\n",
              "      <td>-2.166981</td>\n",
              "      <td>3.105842</td>\n",
              "      <td>-0.451497</td>\n",
              "      <td>-2.087396</td>\n",
              "      <td>0.219262</td>\n",
              "      <td>-1.059650</td>\n",
              "      <td>2.685484</td>\n",
              "      <td>-0.943232</td>\n",
              "      <td>-2.120470</td>\n",
              "      <td>0.191212</td>\n",
              "      <td>0.415367</td>\n",
              "      <td>-0.152114</td>\n",
              "      <td>-0.974302</td>\n",
              "      <td>1.787914</td>\n",
              "      <td>0.364055</td>\n",
              "      <td>2.367908</td>\n",
              "      <td>2.612651</td>\n",
              "      <td>-1.235037</td>\n",
              "      <td>0.640992</td>\n",
              "      <td>-0.342927</td>\n",
              "      <td>1.212635</td>\n",
              "      <td>-0.657298</td>\n",
              "      <td>0.674154</td>\n",
              "      <td>-0.255980</td>\n",
              "      <td>-4.612574</td>\n",
              "      <td>-0.206135</td>\n",
              "      <td>0.060253</td>\n",
              "      <td>-0.206599</td>\n",
              "      <td>-0.530276</td>\n",
              "      <td>-0.092508</td>\n",
              "      <td>0.270761</td>\n",
              "      <td>-0.348374</td>\n",
              "      <td>-0.052450</td>\n",
              "      <td>-0.162833</td>\n",
              "      <td>0.291443</td>\n",
              "      <td>0.158516</td>\n",
              "      <td>-0.568759</td>\n",
              "      <td>-0.187536</td>\n",
              "      <td>0.397226</td>\n",
              "      <td>0.076472</td>\n",
              "      <td>0.231934</td>\n",
              "      <td>0.490374</td>\n",
              "      <td>-0.578810</td>\n",
              "      <td>-0.747969</td>\n",
              "      <td>0.173777</td>\n",
              "      <td>0.228035</td>\n",
              "      <td>0.326183</td>\n",
              "      <td>-0.095992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.236834</td>\n",
              "      <td>0.447515</td>\n",
              "      <td>0.942494</td>\n",
              "      <td>1.831178</td>\n",
              "      <td>4.035197</td>\n",
              "      <td>-2.715159</td>\n",
              "      <td>-0.494452</td>\n",
              "      <td>-1.390876</td>\n",
              "      <td>0.988349</td>\n",
              "      <td>-0.324750</td>\n",
              "      <td>2.090473</td>\n",
              "      <td>0.823172</td>\n",
              "      <td>0.336053</td>\n",
              "      <td>-0.798366</td>\n",
              "      <td>-4.623056</td>\n",
              "      <td>-1.926243</td>\n",
              "      <td>-3.884985</td>\n",
              "      <td>3.242795</td>\n",
              "      <td>1.386794</td>\n",
              "      <td>-1.520089</td>\n",
              "      <td>-0.965930</td>\n",
              "      <td>-3.402725</td>\n",
              "      <td>1.618080</td>\n",
              "      <td>-1.161801</td>\n",
              "      <td>-0.997457</td>\n",
              "      <td>-1.924520</td>\n",
              "      <td>2.012156</td>\n",
              "      <td>0.129474</td>\n",
              "      <td>-0.090356</td>\n",
              "      <td>-1.096475</td>\n",
              "      <td>0.341787</td>\n",
              "      <td>0.791985</td>\n",
              "      <td>0.778682</td>\n",
              "      <td>-0.449166</td>\n",
              "      <td>0.449953</td>\n",
              "      <td>-0.095116</td>\n",
              "      <td>-0.952586</td>\n",
              "      <td>0.459136</td>\n",
              "      <td>1.150347</td>\n",
              "      <td>-0.370117</td>\n",
              "      <td>-0.104506</td>\n",
              "      <td>0.267578</td>\n",
              "      <td>-0.497688</td>\n",
              "      <td>0.174427</td>\n",
              "      <td>0.314535</td>\n",
              "      <td>-0.460330</td>\n",
              "      <td>0.096316</td>\n",
              "      <td>0.964550</td>\n",
              "      <td>-0.072824</td>\n",
              "      <td>-1.039220</td>\n",
              "      <td>0.396921</td>\n",
              "      <td>-0.054091</td>\n",
              "      <td>0.634633</td>\n",
              "      <td>-0.134242</td>\n",
              "      <td>-0.533577</td>\n",
              "      <td>-0.755789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.658501</td>\n",
              "      <td>-1.475041</td>\n",
              "      <td>0.748745</td>\n",
              "      <td>-1.426473</td>\n",
              "      <td>-3.699597</td>\n",
              "      <td>-1.634000</td>\n",
              "      <td>-2.526913</td>\n",
              "      <td>1.611579</td>\n",
              "      <td>0.905675</td>\n",
              "      <td>-0.149053</td>\n",
              "      <td>2.128803</td>\n",
              "      <td>1.691138</td>\n",
              "      <td>-0.324888</td>\n",
              "      <td>-0.908450</td>\n",
              "      <td>1.949752</td>\n",
              "      <td>0.952909</td>\n",
              "      <td>-2.622906</td>\n",
              "      <td>-1.713309</td>\n",
              "      <td>0.754372</td>\n",
              "      <td>-0.925261</td>\n",
              "      <td>0.833147</td>\n",
              "      <td>-0.777138</td>\n",
              "      <td>-1.055742</td>\n",
              "      <td>0.722648</td>\n",
              "      <td>-2.361380</td>\n",
              "      <td>-0.383122</td>\n",
              "      <td>-0.511467</td>\n",
              "      <td>-0.363378</td>\n",
              "      <td>-0.724928</td>\n",
              "      <td>-0.183622</td>\n",
              "      <td>0.361760</td>\n",
              "      <td>0.512733</td>\n",
              "      <td>4.465670</td>\n",
              "      <td>0.089012</td>\n",
              "      <td>0.133642</td>\n",
              "      <td>0.174444</td>\n",
              "      <td>0.723054</td>\n",
              "      <td>-0.351850</td>\n",
              "      <td>-0.112913</td>\n",
              "      <td>0.225106</td>\n",
              "      <td>-0.012036</td>\n",
              "      <td>0.409238</td>\n",
              "      <td>-0.554399</td>\n",
              "      <td>0.402663</td>\n",
              "      <td>0.550017</td>\n",
              "      <td>-0.153775</td>\n",
              "      <td>-0.121379</td>\n",
              "      <td>-0.276993</td>\n",
              "      <td>0.304675</td>\n",
              "      <td>-0.380483</td>\n",
              "      <td>0.013872</td>\n",
              "      <td>0.904209</td>\n",
              "      <td>-0.299967</td>\n",
              "      <td>0.329411</td>\n",
              "      <td>0.069959</td>\n",
              "      <td>-0.408021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.748999</td>\n",
              "      <td>1.833417</td>\n",
              "      <td>-2.441207</td>\n",
              "      <td>-0.946759</td>\n",
              "      <td>2.490193</td>\n",
              "      <td>-4.485895</td>\n",
              "      <td>-2.535448</td>\n",
              "      <td>-2.598420</td>\n",
              "      <td>-1.294683</td>\n",
              "      <td>-1.386219</td>\n",
              "      <td>-1.564114</td>\n",
              "      <td>-0.846953</td>\n",
              "      <td>0.879919</td>\n",
              "      <td>2.339151</td>\n",
              "      <td>-1.867399</td>\n",
              "      <td>1.283344</td>\n",
              "      <td>1.502694</td>\n",
              "      <td>-3.004918</td>\n",
              "      <td>-0.397677</td>\n",
              "      <td>0.129223</td>\n",
              "      <td>-3.509833</td>\n",
              "      <td>1.425902</td>\n",
              "      <td>0.361515</td>\n",
              "      <td>0.774268</td>\n",
              "      <td>-1.216181</td>\n",
              "      <td>3.697596</td>\n",
              "      <td>0.882637</td>\n",
              "      <td>-0.627216</td>\n",
              "      <td>-0.160732</td>\n",
              "      <td>-0.582654</td>\n",
              "      <td>0.298953</td>\n",
              "      <td>0.306886</td>\n",
              "      <td>-0.629769</td>\n",
              "      <td>-0.199482</td>\n",
              "      <td>-0.321718</td>\n",
              "      <td>0.176495</td>\n",
              "      <td>0.329783</td>\n",
              "      <td>-0.545061</td>\n",
              "      <td>0.351163</td>\n",
              "      <td>-0.011748</td>\n",
              "      <td>-0.224851</td>\n",
              "      <td>0.035192</td>\n",
              "      <td>0.157220</td>\n",
              "      <td>0.375420</td>\n",
              "      <td>-0.353154</td>\n",
              "      <td>0.003838</td>\n",
              "      <td>0.033852</td>\n",
              "      <td>-0.421979</td>\n",
              "      <td>-0.318678</td>\n",
              "      <td>-0.069865</td>\n",
              "      <td>-0.041326</td>\n",
              "      <td>0.035976</td>\n",
              "      <td>-0.313842</td>\n",
              "      <td>-0.428427</td>\n",
              "      <td>0.038867</td>\n",
              "      <td>0.614626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.833144</td>\n",
              "      <td>-1.303949</td>\n",
              "      <td>4.043544</td>\n",
              "      <td>-0.219405</td>\n",
              "      <td>2.552707</td>\n",
              "      <td>-2.697711</td>\n",
              "      <td>-2.173731</td>\n",
              "      <td>-0.637294</td>\n",
              "      <td>1.118104</td>\n",
              "      <td>0.177372</td>\n",
              "      <td>-0.791029</td>\n",
              "      <td>-2.880402</td>\n",
              "      <td>1.589534</td>\n",
              "      <td>1.483211</td>\n",
              "      <td>1.656625</td>\n",
              "      <td>-0.573594</td>\n",
              "      <td>-0.920326</td>\n",
              "      <td>-0.048768</td>\n",
              "      <td>2.529475</td>\n",
              "      <td>-0.892568</td>\n",
              "      <td>0.557739</td>\n",
              "      <td>-0.013900</td>\n",
              "      <td>0.060773</td>\n",
              "      <td>0.489898</td>\n",
              "      <td>2.318283</td>\n",
              "      <td>-1.098597</td>\n",
              "      <td>-0.217817</td>\n",
              "      <td>-1.117312</td>\n",
              "      <td>0.883580</td>\n",
              "      <td>-0.099005</td>\n",
              "      <td>0.751728</td>\n",
              "      <td>0.169300</td>\n",
              "      <td>-3.689019</td>\n",
              "      <td>-0.402877</td>\n",
              "      <td>-0.259092</td>\n",
              "      <td>-0.304338</td>\n",
              "      <td>0.335484</td>\n",
              "      <td>0.279515</td>\n",
              "      <td>-0.072168</td>\n",
              "      <td>0.276214</td>\n",
              "      <td>-0.273628</td>\n",
              "      <td>-0.182982</td>\n",
              "      <td>-0.109527</td>\n",
              "      <td>0.335881</td>\n",
              "      <td>0.285368</td>\n",
              "      <td>0.264924</td>\n",
              "      <td>0.129178</td>\n",
              "      <td>0.119394</td>\n",
              "      <td>0.143332</td>\n",
              "      <td>-0.385476</td>\n",
              "      <td>-0.111564</td>\n",
              "      <td>-0.854590</td>\n",
              "      <td>-0.287042</td>\n",
              "      <td>0.624932</td>\n",
              "      <td>-0.623190</td>\n",
              "      <td>0.550105</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        53        54        55\n",
              "0 -2.206029 -0.808622  0.227027  ... -1.070026 -0.285900  0.443744\n",
              "1  1.682056 -0.609567 -2.252846  ...  0.191268 -0.078609  0.022873\n",
              "2  2.058921 -1.012089 -4.200319  ...  0.271430  0.086816  0.233399\n",
              "3  1.496926  3.138901 -2.547379  ... -0.039403 -0.195077 -0.499018\n",
              "4  0.791522 -4.199571 -4.615912  ... -0.107969  0.086815  0.554617\n",
              "5  1.041741  0.082437  0.549398  ...  0.228035  0.326183 -0.095992\n",
              "6  0.236834  0.447515  0.942494  ... -0.134242 -0.533577 -0.755789\n",
              "7  0.658501 -1.475041  0.748745  ...  0.329411  0.069959 -0.408021\n",
              "8  1.748999  1.833417 -2.441207  ... -0.428427  0.038867  0.614626\n",
              "9  1.833144 -1.303949  4.043544  ...  0.624932 -0.623190  0.550105\n",
              "\n",
              "[10 rows x 56 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avXnOi9WH9tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}